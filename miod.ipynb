{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, StackingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Input\n",
    "from statsmodels.tsa.arima.model import ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "file_path = '/workspaces/codespaces-jupyter/Medical Inventory Optimization Dataset - Cleaned.csv'\n",
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Dateofbill to datetime\n",
    "data['Dateofbill'] = pd.to_datetime(data['Dateofbill'])\n",
    "\n",
    "# Aggregate Quantity and Final_Sales by DrugName, Dateofbill, Specialisation, and Dept\n",
    "aggregated_data = data.groupby(['DrugName', 'Dateofbill', 'Specialisation', 'Dept']).agg({\n",
    "    'Quantity': 'sum',\n",
    "    'Final_Sales': 'sum'\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "# Extract date features\n",
    "aggregated_data['Month'] = aggregated_data['Dateofbill'].dt.month\n",
    "aggregated_data['Day'] = aggregated_data['Dateofbill'].dt.day\n",
    "aggregated_data['Year'] = aggregated_data['Dateofbill'].dt.year\n",
    "aggregated_data['Is_Weekend'] = aggregated_data['Dateofbill'].dt.dayofweek >= 5\n",
    "\n",
    "# Cyclic features for Month\n",
    "aggregated_data['Month_sin'] = np.sin(2 * np.pi * aggregated_data['Month'] / 12)\n",
    "aggregated_data['Month_cos'] = np.cos(2 * np.pi * aggregated_data['Month'] / 12)\n",
    "\n",
    "# Lag features for Quantity and Final_Sales\n",
    "aggregated_data['Lag_7'] = aggregated_data.groupby('DrugName')['Final_Sales'].shift(7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate rolling averages for Quantity and Final_Sales\n",
    "aggregated_data['Quantity_MA'] = aggregated_data.groupby('DrugName')['Quantity'].transform(lambda x: x.rolling(window=7, min_periods=1).mean())\n",
    "aggregated_data['Final_Sales_MA'] = aggregated_data.groupby('DrugName')['Final_Sales'].transform(lambda x: x.rolling(window=7, min_periods=1).mean())\n",
    "\n",
    "# Drop NaN values after creating lag features\n",
    "aggregated_data.dropna(inplace=True)\n",
    "\n",
    "# Prepare data for modeling\n",
    "X = aggregated_data[['Month_sin', 'Month_cos', 'Day', 'Year', 'Is_Weekend', 'Lag_7', 'Quantity_MA', 'Final_Sales_MA']]\n",
    "y = aggregated_data['Final_Sales']\n",
    "\n",
    "# Split data into training and testing sets using TimeSeriesSplit for cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "for train_index, test_index in tscv.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 1. Linear Regression with Ridge Regularization\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "ridge_model.fit(X_train, y_train)\n",
    "ridge_predictions = ridge_model.predict(X_test)\n",
    "ridge_rmse = mean_squared_error(y_test, ridge_predictions, squared=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 2. Random Forest with Hyperparameter Tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "grid_search = GridSearchCV(RandomForestRegressor(), param_grid, cv=tscv, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "rf_predictions = best_rf_model.predict(X_test)\n",
    "rf_rmse = mean_squared_error(y_test, rf_predictions, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 392425.3125\n",
      "Epoch 2/10\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 355692.9062\n",
      "Epoch 3/10\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 348775.2812\n",
      "Epoch 4/10\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 341629.8438\n",
      "Epoch 5/10\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 300236.6875\n",
      "Epoch 6/10\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 358179.6875\n",
      "Epoch 7/10\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 383795.0938\n",
      "Epoch 8/10\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 384680.1562\n",
      "Epoch 9/10\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 347396.6562\n",
      "Epoch 10/10\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 370441.4375\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 3. LSTM Model\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train_scaled, X_test_scaled = X_scaled[train_index], X_scaled[test_index]\n",
    "\n",
    "X_train_lstm = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "X_test_lstm = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(Input(shape=(1, X_train_scaled.shape[1])))\n",
    "lstm_model.add(LSTM(50, return_sequences=True))\n",
    "lstm_model.add(LSTM(50))\n",
    "lstm_model.add(Dense(1))\n",
    "lstm_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "lstm_model.fit(X_train_lstm, y_train, epochs=10, batch_size=32)\n",
    "lstm_predictions = lstm_model.predict(X_test_lstm)\n",
    "lstm_rmse = mean_squared_error(y_test, lstm_predictions, squared=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 4. Stacking Regressor (Combining Ridge and Random Forest)\n",
    "estimators = [\n",
    "    ('rf', best_rf_model),\n",
    "    ('ridge', ridge_model)\n",
    "]\n",
    "\n",
    "stacking_model = StackingRegressor(estimators=estimators, final_estimator=LinearRegression())\n",
    "stacking_model.fit(X_train, y_train)\n",
    "stacking_predictions = stacking_model.predict(X_test)\n",
    "stacking_rmse = mean_squared_error(y_test, stacking_predictions, squared=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression RMSE: 230.68938445685066\n",
      "Random Forest RMSE (with hyperparameter tuning): 231.38696831791785\n",
      "LSTM RMSE: 373.09990166703517\n",
      "Stacking Regressor RMSE: 230.6648075920067\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print RMSE for each model\n",
    "print(f'Ridge Regression RMSE: {ridge_rmse}')\n",
    "print(f'Random Forest RMSE (with hyperparameter tuning): {rf_rmse}')\n",
    "print(f'LSTM RMSE: {lstm_rmse}')\n",
    "print(f'Stacking Regressor RMSE: {stacking_rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate last known moving averages and lag for future predictions\n",
    "last_known_date = aggregated_data['Dateofbill'].max()\n",
    "last_known_quantity_ma = aggregated_data.loc[aggregated_data['Dateofbill'] == last_known_date, 'Quantity_MA'].values[0]\n",
    "last_known_final_sales_ma = aggregated_data.loc[aggregated_data['Dateofbill'] == last_known_date, 'Final_Sales_MA'].values[0]\n",
    "last_known_lag_7 = aggregated_data.loc[aggregated_data['Dateofbill'] == last_known_date, 'Lag_7'].values[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict future sales using the best model (e.g., Stacking)\n",
    "future_dates = pd.date_range(start=last_known_date, periods=30, freq='D')\n",
    "future_data = pd.DataFrame({\n",
    "    'Dateofbill': future_dates,\n",
    "    'Month_sin': np.sin(2 * np.pi * future_dates.month / 12),\n",
    "    'Month_cos': np.cos(2 * np.pi * future_dates.month / 12),\n",
    "    'Day': future_dates.day,\n",
    "    'Year': future_dates.year,\n",
    "    'Is_Weekend': future_dates.dayofweek >= 5,\n",
    "    'Lag_7': last_known_lag_7,\n",
    "    'Quantity_MA': last_known_quantity_ma,\n",
    "    'Final_Sales_MA': last_known_final_sales_ma\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Dateofbill  Predicted_Sales  Optimal_Stock_Level\n",
      "0  2022-12-31       300.138249           330.152074\n",
      "1  2023-01-01       294.915775           324.407353\n",
      "2  2023-01-02       290.433817           319.477199\n",
      "3  2023-01-03       290.611279           319.672406\n",
      "4  2023-01-04       290.756168           319.831785\n",
      "5  2023-01-05       290.944992           320.039491\n",
      "6  2023-01-06       290.975642           320.073206\n",
      "7  2023-01-07       295.550719           325.105791\n",
      "8  2023-01-08       295.628144           325.190958\n",
      "9  2023-01-09       291.271056           320.398162\n",
      "10 2023-01-10       291.327352           320.460087\n",
      "11 2023-01-11       291.476562           320.624218\n",
      "12 2023-01-12       291.610457           320.771503\n",
      "13 2023-01-13       291.705635           320.876198\n",
      "14 2023-01-14       296.306339           325.936973\n",
      "15 2023-01-15       296.435122           326.078634\n",
      "16 2023-01-16       292.088823           321.297705\n",
      "17 2023-01-17       292.140815           321.354897\n",
      "18 2023-01-18       292.343369           321.577706\n",
      "19 2023-01-19       292.345483           321.580031\n",
      "20 2023-01-20       292.392691           321.631960\n",
      "21 2023-01-21       297.093515           326.802867\n",
      "22 2023-01-22       297.187504           326.906254\n",
      "23 2023-01-23       292.727125           321.999837\n",
      "24 2023-01-24       292.891490           322.180639\n",
      "25 2023-01-25       292.969590           322.266549\n",
      "26 2023-01-26       293.074523           322.381975\n",
      "27 2023-01-27       293.005554           322.306110\n",
      "28 2023-01-28       297.592682           327.351950\n",
      "29 2023-01-29       297.618922           327.380814\n"
     ]
    }
   ],
   "source": [
    "# Scale and reshape future data for LSTM\n",
    "future_X = future_data[['Month_sin', 'Month_cos', 'Day', 'Year', 'Is_Weekend', 'Lag_7', 'Quantity_MA', 'Final_Sales_MA']]\n",
    "future_X_scaled = scaler.transform(future_X)\n",
    "future_X_lstm = future_X_scaled.reshape((future_X_scaled.shape[0], 1, future_X_scaled.shape[1]))\n",
    "\n",
    "future_predictions = stacking_model.predict(future_X)  # or use lstm_model for future prediction\n",
    "optimal_stock_levels = future_predictions * 1.1  # Adding 10% buffer\n",
    "\n",
    "# Display future predictions and optimal stock levels\n",
    "future_data['Predicted_Sales'] = future_predictions\n",
    "future_data['Optimal_Stock_Level'] = optimal_stock_levels\n",
    "\n",
    "print(future_data[['Dateofbill', 'Predicted_Sales', 'Optimal_Stock_Level']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
